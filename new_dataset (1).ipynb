{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733695e9-c7d4-4d74-a2d0-7b324dbf98d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "# Dataset directory\n",
    "base_dir = r\"D:\\dataset\\dataset\\New folder\"  \n",
    "img_size = (128, 128)  # Increased for better feature extraction\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "if not os.path.exists(base_dir) or len(os.listdir(base_dir)) == 0:\n",
    "    print(\"Skipping training: Directory not found or empty.\")\n",
    "else:\n",
    "    def check_class_distribution(base_dir):\n",
    "        \"\"\"Check the number of images in each class.\"\"\"\n",
    "        class_counts = {cls: len(os.listdir(os.path.join(base_dir, cls))) for cls in os.listdir(base_dir)}\n",
    "        print(\"Class distribution:\", class_counts)\n",
    "        return class_counts\n",
    "\n",
    "    class_counts = check_class_distribution(base_dir)\n",
    "\n",
    "    def load_data():\n",
    "        \"\"\"Load and preprocess training and validation data.\"\"\"\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=30,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            shear_range=0.3,\n",
    "            zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # No augmentation for validation\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            base_dir,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",  \n",
    "            subset=\"training\"\n",
    "        )\n",
    "\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            base_dir,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "            subset=\"validation\"\n",
    "        )\n",
    "\n",
    "        # Save class mappings\n",
    "        class_indices = train_generator.class_indices\n",
    "        with open(\"class_indices.json\", \"w\") as f:\n",
    "            json.dump(class_indices, f)\n",
    "        print(\"Saved class mappings:\", class_indices)\n",
    "\n",
    "        return train_generator, val_generator\n",
    "\n",
    "    def compute_class_weights(train_generator):\n",
    "        \"\"\"Compute class weights to handle imbalanced data.\"\"\"\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_generator.classes),\n",
    "            y=train_generator.classes\n",
    "        )\n",
    "        return dict(enumerate(class_weights))\n",
    "\n",
    "    def create_model(input_shape, num_classes):\n",
    "        \"\"\"Define a CNN using MobileNetV2.\"\"\"\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Load Data\n",
    "    train_generator, val_generator = load_data()\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    \n",
    "    # Compute Class Weights\n",
    "    class_weights = compute_class_weights(train_generator)\n",
    "\n",
    "    # Build Model\n",
    "    model = create_model((img_size[0], img_size[1], 3), num_classes)\n",
    "\n",
    "    # Define Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
    "        ModelCheckpoint(\"sign_language_best_model1.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "    ]\n",
    "\n",
    "    # Train Model\n",
    "    model.fit(train_generator, \n",
    "              validation_data=val_generator, \n",
    "              epochs=epochs, \n",
    "              class_weight=class_weights,  \n",
    "              callbacks=callbacks, \n",
    "              steps_per_epoch=len(train_generator),\n",
    "              validation_steps=len(val_generator))\n",
    "\n",
    "    # Save Final Model\n",
    "    model.save(\"sign_language_model1.keras\")\n",
    "    print(\"Model training completed and saved as sign_language_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31950f46-bd8d-4172-8eb0-7a811949ed44",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load trained model and labels\n",
    "def load_trained_model():\n",
    "    \"\"\"Loads the trained model and the corresponding class labels.\"\"\"\n",
    "    try:\n",
    "        model = load_model(\"sign_language_model1.keras\")\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model file not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(\"class_indices.json\", \"r\") as f:\n",
    "            class_indices = json.load(f)\n",
    "        class_labels = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "        print(\"Loaded class labels:\", class_labels)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Class label file not found. Using default labels.\")\n",
    "        class_labels = {0: \"Unknown\", 1: \"Unknown\"}\n",
    "\n",
    "    return model, class_labels\n",
    "\n",
    "# Load model and labels\n",
    "model, class_labels = load_trained_model()\n",
    "if model is None:\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def real_time_prediction(model, class_labels):\n",
    "    \"\"\"Perform real-time predictions from webcam input.\"\"\"\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, c = frame.shape\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    x_min, y_min = min(x, x_min), min(y, y_min)\n",
    "                    x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "                padding = 40\n",
    "                x_min, y_min = max(x_min - padding, 0), max(y_min - padding, 0)\n",
    "                x_max, y_max = min(x_max + padding, w), min(y_max + padding, h)\n",
    "\n",
    "                if x_max - x_min > 10 and y_max - y_min > 10:\n",
    "                    hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                    target_size = model.input_shape[1:3]\n",
    "                    hand_roi = cv2.resize(hand_roi, target_size)\n",
    "                    hand_roi = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2RGB)\n",
    "                    hand_roi = img_to_array(hand_roi) / 255.0  \n",
    "                    hand_roi = np.expand_dims(hand_roi, axis=0)\n",
    "\n",
    "                    predictions = model.predict(hand_roi)[0]\n",
    "                    confidence = np.max(predictions)\n",
    "                    predicted_class = np.argmax(predictions)\n",
    "                    predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
    "\n",
    "                    if confidence > 0.7:  # Adjust threshold\n",
    "                        color = (0, 255, 0)\n",
    "                    else:\n",
    "                        predicted_label = \"No sign detected\"\n",
    "                        color = (0, 0, 255)\n",
    "\n",
    "                    cv2.putText(frame, f\"Prediction: {predicted_label} ({confidence:.2f})\",\n",
    "                                (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"ASL Hand Tracking\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Perform real-time prediction\n",
    "real_time_prediction(model, class_labels)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87249390-07cc-4d49-b29a-6eed0a78e830",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\mediapipe\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 - 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\mediapipe\\python\\solutions\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Solutions Python API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_styles\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_utils\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_detection\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\mediapipe\\python\\solutions\\drawing_styles.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m face_mesh_connections\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hands_connections\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DrawingSpec\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhands\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HandLandmark\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PoseLandmark\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\mediapipe\\python\\solutions\\drawing_utils.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Mapping, Optional, Tuple, Union\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detection_pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\pyplot.py:66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _docstring\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Figure, FigureBase, figaspect\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\figure.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     44\u001b[0m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\projections\\__init__.py:55\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m axes, _docstring\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\axes\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _base\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_axes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Axes  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Backcompat.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m Subplot \u001b[38;5;241m=\u001b[39m Axes\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\axes\\_axes.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmticker\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmtransforms\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtri\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmtri\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munits\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmunits\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring, _preprocess_data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\tri\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tricontour\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TriContourSet, tricontour, tricontourf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trifinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TriFinder, TrapezoidMapTriFinder\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_triinterpolate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (TriInterpolator, LinearTriInterpolator,\n\u001b[0;32m      9\u001b[0m                               CubicTriInterpolator)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tripcolor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tripcolor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_triplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m triplot\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\matplotlib\\tri\\_triinterpolate.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtri\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Triangulation\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtri\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trifinder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TriFinder\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtri\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tritools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TriAnalyzer\n\u001b[0;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTriInterpolator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearTriInterpolator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCubicTriInterpolator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTriInterpolator\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load trained model and labels\n",
    "def load_trained_model():\n",
    "    \"\"\"Loads the trained model and the corresponding class labels.\"\"\"\n",
    "    try:\n",
    "        model = load_model(\"sign_language_model1.keras\")\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model file not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(\"class_indices.json\", \"r\") as f:\n",
    "            class_indices = json.load(f)\n",
    "        class_labels = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "        print(\"Loaded class labels:\", class_labels)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Class label file not found. Using default labels.\")\n",
    "        class_labels = {0: \"Unknown\", 1: \"Unknown\"}\n",
    "\n",
    "    return model, class_labels\n",
    "\n",
    "# Load model and labels\n",
    "model, class_labels = load_trained_model()\n",
    "if model is None:\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_hand_box_and_predict(model, class_labels):\n",
    "    \"\"\"Detects and draws a bounding box around the hand, and uses the model for predictions.\"\"\"\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, c = frame.shape\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get bounding box coordinates\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    x_min, y_min = min(x, x_min), min(y, y_min)\n",
    "                    x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "                # Add padding around the hand\n",
    "                padding = 40\n",
    "                x_min, y_min = max(x_min - padding, 0), max(y_min - padding, 0)\n",
    "                x_max, y_max = min(x_max + padding, w), min(y_max + padding, h)\n",
    "\n",
    "                # Draw the bounding box around the hand\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Extract the hand ROI (Region of Interest) from the frame\n",
    "                hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                # Preprocess the hand ROI for the model\n",
    "                target_size = model.input_shape[1:3]  # Size your model expects\n",
    "                hand_roi_resized = cv2.resize(hand_roi, target_size)\n",
    "                hand_roi_resized = cv2.cvtColor(hand_roi_resized, cv2.COLOR_BGR2RGB)\n",
    "                hand_roi_resized = img_to_array(hand_roi_resized) / 255.0\n",
    "                hand_roi_resized = np.expand_dims(hand_roi_resized, axis=0)\n",
    "\n",
    "                # Predict using the model (optional)\n",
    "                predictions = model.predict(hand_roi_resized)\n",
    "                predicted_class = np.argmax(predictions)\n",
    "                predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
    "                confidence = np.max(predictions)\n",
    "\n",
    "                # Display prediction label if confidence is high\n",
    "                if confidence > 0.7:\n",
    "                    color = (0, 255, 0)  # Green for high confidence\n",
    "                    cv2.putText(frame, f\"Prediction: {predicted_label} ({confidence:.2f})\",\n",
    "                                (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                else:\n",
    "                    color = (0, 0, 255)  # Red for low confidence\n",
    "                    cv2.putText(frame, \"No clear prediction\", (x_min, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                # Optionally, you can also draw the hand landmarks\n",
    "                # mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the frame with the bounding box and prediction\n",
    "        cv2.imshow(\"Hand Detection and Prediction\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Perform hand detection, draw bounding box, and prediction\n",
    "detect_hand_box_and_predict(model, class_labels)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
