{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92183d-b36c-49b9-8175-b669e5b5f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load trained model and labels\n",
    "def load_trained_model():\n",
    "    \"\"\"Loads the trained model and the corresponding class labels.\"\"\"\n",
    "    try:\n",
    "        model = load_model(\"sign_language_model1.keras\")\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model file not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(\"class_indices.json\", \"r\") as f:\n",
    "            class_indices = json.load(f)\n",
    "        class_labels = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
    "        print(\"Loaded class labels:\", class_labels)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Class label file not found. Using default labels.\")\n",
    "        class_labels = {0: \"Unknown\", 1: \"Unknown\"}\n",
    "\n",
    "    return model, class_labels\n",
    "\n",
    "# Load model and labels\n",
    "model, class_labels = load_trained_model()\n",
    "if model is None:\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def detect_hand_box_and_predict(model, class_labels):\n",
    "    \"\"\"Detects and draws a bounding box around the hand, and uses the model for predictions.\"\"\"\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, c = frame.shape\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get bounding box coordinates\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    x_min, y_min = min(x, x_min), min(y, y_min)\n",
    "                    x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "                # Add padding around the hand\n",
    "                padding = 40\n",
    "                x_min, y_min = max(x_min - padding, 0), max(y_min - padding, 0)\n",
    "                x_max, y_max = min(x_max + padding, w), min(y_max + padding, h)\n",
    "\n",
    "                # Draw the bounding box around the hand\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Extract the hand ROI (Region of Interest) from the frame\n",
    "                hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                # Preprocess the hand ROI for the model\n",
    "                target_size = model.input_shape[1:3]  # Size your model expects\n",
    "                hand_roi_resized = cv2.resize(hand_roi, target_size)\n",
    "                hand_roi_resized = cv2.cvtColor(hand_roi_resized, cv2.COLOR_BGR2RGB)\n",
    "                hand_roi_resized = img_to_array(hand_roi_resized) / 255.0\n",
    "                hand_roi_resized = np.expand_dims(hand_roi_resized, axis=0)\n",
    "\n",
    "                # Predict using the model (optional)\n",
    "                predictions = model.predict(hand_roi_resized)\n",
    "                predicted_class = np.argmax(predictions)\n",
    "                predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
    "                confidence = np.max(predictions)\n",
    "\n",
    "                # Display prediction label if confidence is high\n",
    "                if confidence > 0.7:\n",
    "                    color = (0, 255, 0)  # Green for high confidence\n",
    "                    cv2.putText(frame, f\"Prediction: {predicted_label} ({confidence:.2f})\",\n",
    "                                (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "                else:\n",
    "                    color = (0, 0, 255)  # Red for low confidence\n",
    "                    cv2.putText(frame, \"No clear prediction\", (x_min, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                # Optionally, you can also draw the hand landmarks\n",
    "                # mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the frame with the bounding box and prediction\n",
    "        cv2.imshow(\"Hand Detection and Prediction\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Perform hand detection, draw bounding box, and prediction\n",
    "detect_hand_box_and_predict(model, class_labels)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999a54a-f41a-4620-b361-14cfc003b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5900f3c-721b-4fb6-895f-6947eabff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7369e-f7d7-400f-9f5f-b554bee2f32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde37f24-54ef-4da0-b1db-f3bfe045adc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe3cf1-76e3-475a-969f-7081cf952521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
